{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Process tif images\n",
    "\n",
    "**Load Spark library**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load pattern libaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import statistics\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initilize Spark and Spark context**\n",
    "<div>Configure memory</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Image_process\") \\\n",
    "    .config(\"spark.executor.memory\", \"20gb\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD to iterate the directory and collect tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration() \n",
    "def listtifFiles(hdfspath, lft):\n",
    "    path = hadoop.fs.Path(hdfspath)\n",
    "    ld = fs.get(conf).listStatus(path)\n",
    "    for d1 in ld:\n",
    "        if fs.get(conf).isDirectory(d1.getPath()):\n",
    "            #print('dir: ' + str(d1.getPath()))\n",
    "            listtifFiles(str(d1.getPath()), lft)\n",
    "        else:\n",
    "            if str(d1.getPath()).endswith('.tif') :\n",
    "                #print('tif: ' + str(d1.getPath()))\n",
    "                lft.append(str(d1.getPath()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration() \n",
    "path = hadoop.fs.Path('hdfs://base directory')\n",
    "lf = [f.getPath() for f in fs.get(conf).listStatus(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf2 = lf#[50:75]\n",
    "lftif = []\n",
    "for d1 in lf2:\n",
    "    #print(str(d1))\n",
    "    listtifFiles(str(d1), lftif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lftif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lftif[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_file = lftif[0]\n",
    "tif_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Process image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_loc = tif_file\n",
    "#binaryRdd=sc.binaryFiles(img_loc)\n",
    "#bin_data=binaryRdd.map(read_array)\n",
    "#img_data = bin_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD-1: Load tif files as binary files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_file_path = 'hdfs://vase directory/*/'\n",
    "binaryRawRdd = sc.binaryFiles(tif_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure to convert binary to numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array(rdd):\n",
    "    np_array = np.frombuffer(rdd[1][:], dtype=np.uint8)\n",
    "    return rdd[0], np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD-2: Convert binary data to numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_np_array_data=binaryRawRdd.map(read_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excute RDDs and collect fist two elements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_data = bin_np_array_data.take(2)\n",
    "#print(process_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add required python file and config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('config.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('CustomMLProcess.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "unique_id = uuid.uuid1()\n",
    "broadcastUUID = sc.broadcast([unique_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add import statement, so that added files is available at worker nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customMLProcess as cml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Procedure to ML process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(rdd):\n",
    "    image_file = rdd[0]\n",
    "    print(image_file)\n",
    "    print(rdd[1])\n",
    "    split_image_file = image_file.split('/')\n",
    "    experiment_number = split_image_file[-3]\n",
    "    disk = split_image_file[-2]\n",
    "    date_str = split_image_file[-4]\n",
    "    #create object\n",
    "    custom_ml_process = cml.customMLProcess()\n",
    "    #load config file\n",
    "    config_file = SparkFiles.get('config.txt')\n",
    "    #process and get process data\n",
    "    csv_ml_data = custom_ml_process.image_process('Location', image_file, rdd[1], config_file)\n",
    "    uuid = broadcastUUID.value[0] \n",
    "    return (uuid, image_file, experiment_number, disk, csv_ml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD-3: Process image and collect ml data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_json_rdd=bin_np_array_data.map(process_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute Spark lineage and collect defects data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_json_data = processed_json_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_json_data[0][4][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert JSON data to pandas data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_defect_data = json.loads(processed_json_data[0][4][0][0])\n",
    "df_defect = pd.read_json(json_defect_data)\n",
    "df_defect['EXPERIMENT_NUMBER_2'] = processed_json_data[0][1] \n",
    "df_defect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defect.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(df_defect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pandas dataframe to hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(df_defect)\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save dataframe to new hive table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = spark.sql('show databases').show()\n",
    "sd = spark.sql('show tables from gold').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a new table in Hive\n",
    "#df.write.mode(\"overwrite\").saveAsTable(\"test_db.test_table2\")\n",
    "# Show the results using SELECT\n",
    "#spark.sql(\"select * from test_db.test_table2\").show()\n",
    "#df.write.mode(\"append\").saveAsTable(\"test_db.test_table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
